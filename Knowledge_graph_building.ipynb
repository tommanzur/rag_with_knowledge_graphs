{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Nlub8FgurHR-"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain\n",
        "#!pip install wikipedia\n",
        "#!pip install llama-index\n",
        "#!pip install openai\n",
        "#!pip install neo4j\n",
        "#!pip install PyPDFLoader\n",
        "#!pip install pypdf\n",
        "#!pip install spacy-llm\n",
        "#!pip install --upgrade jupyter ipywidgets\n",
        "#!pip install -q llama-index google-generativeai\n",
        "#!pip install spacy-llm\n",
        "#!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from wasabi import msg\n",
        "from spacy_llm.util import assemble\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "1cKpqCADuE9C"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'OPENAI_API_KEY'\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "DAg9u3sgVhmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the OpenAI Wikipedia page\n",
        "\n",
        "raw_documents = WikipediaLoader(query=\"Ordenamiento territorial\").load()\n",
        "\n",
        "# Define chunking strategy\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=20\n",
        ")\n",
        "# Chunk the document\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "for d in documents:\n",
        "    del d.metadata[\"summary\"]\n"
      ],
      "metadata": {
        "id": "z65eHDilrU3r"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing your PDF files\n",
        "directory_path = './data'\n",
        "\n",
        "# Initialize PyPDFLoader for each PDF in the directory\n",
        "loaders = [PyPDFLoader(os.path.join(directory_path, f)) for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
        "\n",
        "# Load documents from PDFs\n",
        "news_docs = []\n",
        "for loader in loaders:\n",
        "    news_docs.extend(loader.load())"
      ],
      "metadata": {
        "id": "tFpKJnPHxZIT"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the content and metadata for each news article as Document objects\n",
        "news_articles_data = [\n",
        "    Document(\n",
        "        page_content=doc.page_content,  # Assuming this is how you access the page content of the document\n",
        "        metadata={\n",
        "            \"source\": doc.metadata['source'].removeprefix('./data/'),  # Assuming this is the metadata format\n",
        "            # Include any other metadata items here\n",
        "        }\n",
        "    )\n",
        "    for doc in news_docs  # Assuming news_docs is a list of objects with page_content and metadata\n",
        "]"
      ],
      "metadata": {
        "id": "3oTl_OXuxoK2"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text splitter\n",
        "rtext_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "\n",
        "# Initialize LLM\n",
        "#model = Gemini(temperature=0.2, model=\"gemini-pro\")\n",
        "model = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\", openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Define the map prompt template\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{all_text_data}\n",
        "Basado en la lista de documentos, por favor realiza resúmenes concisos mientras\n",
        "extraes las relaciones esenciales para el análisis de relaciones posterior. Es importante\n",
        "incluir las fechas de acciones o eventos, ya que son cruciales para el análisis de la\n",
        "línea de tiempo posterior. Ejemplo: 'La junta de planificación territorial de Mendoza\n",
        "aprueba la primera versión del Plan Provincial de Ordenamiento Territorial el 15/03/2014 (15 de marzo, sábado)',\n",
        "lo que no solo muestra la relación entre la junta de planificación y el Plan Provincial de Ordenamiento Territorial,\n",
        "sino también cuándo sucede.\"\"\"\n",
        "\n",
        "\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "print(map_prompt)\n",
        "# Define the map_chain\n",
        "map_chain = LLMChain(llm=model, prompt=map_prompt)\n",
        "\n",
        "all_data = news_articles_data\n",
        "# Extract text from each document\n",
        "all_text_data = [doc.page_content for doc in all_text_data]\n",
        "\n",
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{all_text_data}\n",
        "Take these and distill them into concise summaries, capturing the essence of the conflict, key arguments, socio-technical controversies, and the relationship between various territorial projects discussed between 2012 and 2017. Include significant events, actors involved, and their impacts on the territorial planning policy. Example: \"During the development of the Provincial Territorial Planning Plan from 2012 to 2017, socio-technical controversies emerged, notably around water resource management, involving diverse actors like local communities, agricultural sectors, and mining companies, leading to a reevaluation of the territory's development model by 2017.\"\n",
        "Helpful Answer:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "\n",
        "# ChatPromptTemplate(input_variables=['all_text_data'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['all_text_data'], template='The following is a set of documents:\\n{all_text_data}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])\n",
        "\n",
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=model, prompt=reduce_prompt)\n",
        "\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain,\n",
        "    document_variable_name=\"all_text_data\"  # This should match the variable name in reduce_prompt\n",
        ")\n",
        "\n",
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000,\n",
        ")\n",
        "\n",
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"all_text_data\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0\n",
        ")\n",
        "split_docs = text_splitter.split_documents(all_text_data)\n",
        "\n",
        "# Run the MapReduce Chain\n",
        "summarization_results = map_reduce_chain.run(split_docs)"
      ],
      "metadata": {
        "id": "vlNS_SKUyk6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('summary.txt', 'w') as file:\n",
        "    file.write(str(summarization_results))"
      ],
      "metadata": {
        "id": "mS5SpyNLy2SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traditional spacy NER (Named Recognition Library)\n",
        "def split_document_sent(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents] # referencial\n",
        "\n",
        "# spacy-llm relationship extraction\n",
        "def process_text(nlp, text, verbose=False):\n",
        "    doc = nlp(text)\n",
        "    if verbose:\n",
        "        msg.text(f\"Text: {doc.text}\")\n",
        "        msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
        "        msg.text(\"Relations:\")\n",
        "        for r in doc._.rel:\n",
        "            msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
        "    return doc\n",
        "\n",
        "def run_pipeline(config_path, examples_path=None, verbose=False):\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
        "\n",
        "    nlp = assemble(config_path, overrides={} if examples_path is None else {\"paths.examples\": str(examples_path)})\n",
        "\n",
        "    # Initialize counters and storage\n",
        "    processed_data = []\n",
        "    entity_counts = Counter()\n",
        "    relation_counts = Counter()\n",
        "\n",
        "    # Load your articles and news data here\n",
        "    # all_data = news_articles_data\n",
        "\n",
        "    sents = split_document_sent(summarization_results)\n",
        "    for sent in sents:\n",
        "        doc = process_text(nlp, sent, verbose)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
        "\n",
        "        # Store processed data\n",
        "        processed_data.append({'text': doc.text, 'entities': entities, 'relations': relations})\n",
        "\n",
        "        # Update counters\n",
        "        entity_counts.update([ent[1] for ent in entities])\n",
        "        relation_counts.update([rel[1] for rel in relations])\n",
        "\n",
        "    # Export to JSON\n",
        "    with open('processed_data.json', 'w') as f:\n",
        "        json.dump(processed_data, f)\n",
        "\n",
        "    # Display summary\n",
        "    msg.text(f\"Entity counts: {entity_counts}\")\n",
        "    msg.text(f\"Relation counts: {relation_counts}\")\n",
        "\n",
        "# Set your configuration paths and flags\n",
        "config_path = Path(\"zeroshot.cfg\")\n",
        "examples_path = None  # or None if not using few-shot\n",
        "verbose = True\n",
        "\n",
        "# Run the pipeline\n",
        "file = run_pipeline(config_path, None, verbose)"
      ],
      "metadata": {
        "id": "mtcQGwDwy4xW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}